import { useRef, useState, useEffect } from "react";
import * as faceapi from "face-api.js";
import { Camera } from "react-camera-pro";

const FaceDetection = () => {
  const camera = useRef(null);
  const [image, setImage] = useState(null);
  const [referenceImage, setReferenceImage] = useState(null); // ‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
  const [isOpen, setIsOpen] = useState(false);
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const [matchResult, setMatchResult] = useState(null); // ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
  const canvasInRef = useRef(null);
  const canvasOutRef = useRef(null);

  useEffect(() => {
    const MODEL_URL = 'https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/weights'

    const loadModels = async () => {
      try {
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
          faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
          faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
          faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
        ]);
        console.log("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!");
        setIsModelLoaded(true);
      } catch (error) {
        console.error("‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î:", error);
      }
    };
    loadModels();
  }, []);

  const handleTakePhoto = () => {
    if (!referenceImage) {
      console.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô!");
      return; // ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û
    }

    if (!camera.current) {
      console.error("‚ùå ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°!");
      return;
    }

    const capturedImage = camera.current.takePhoto();
    if (!capturedImage) {
      console.error("‚ùå ‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!");
      return;
    }

    setImage(capturedImage);
  };

  const handleImageLoad = async () => {
    if (!image || !referenceImage) return;
    if (!isModelLoaded) {
      console.error("‚ùå ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à!");
      return;
    }
  
    const img = document.getElementById("taken-photo");
    const refImg = document.getElementById("reference-photo");
    const canvas = canvasOutRef.current;
    if (!img || !refImg || !canvas) return;
  
    const displaySize = { width: img.width, height: img.height };
    faceapi.matchDimensions(canvas, displaySize);
  
    // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì descriptor ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    const referenceDetection = await faceapi
      .detectSingleFace(refImg, new faceapi.TinyFaceDetectorOptions())
      .withFaceLandmarks()
      .withFaceDescriptor();
  
    if (!referenceDetection) {
      console.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö!");
      setMatchResult("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö!")
      return;
    }
  
    // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì descriptor ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ñ‡πà‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
    const capturedDetections = await faceapi
      .detectAllFaces(img, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.8 }))
      .withFaceLandmarks()
      .withFaceDescriptors();
  
    if (capturedDetections.length === 0) {
      console.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ñ‡πà‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á!");
      setMatchResult("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ñ‡πà‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á!");
      return;
    }
  
    // ‡∏™‡∏£‡πâ‡∏≤‡∏á faceMatcher ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    const faceMatcher = new faceapi.FaceMatcher([referenceDetection], 0.4);
  
    // ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    const results = capturedDetections.map((d) =>
      faceMatcher.findBestMatch(d.descriptor)
    );
  
    // ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    const bestMatch = results[0];
    const isMatch = bestMatch.distance < 0.4; // ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0.5

    // ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
    setMatchResult(isMatch ? "‚úÖ ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô" : "‚ùå ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô");
  
    // ‡∏ß‡∏≤‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ö‡∏ô canvas
    const resizedDetections = faceapi.resizeResults(capturedDetections, displaySize);
    const ctx = canvas.getContext("2d");
    if (ctx) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    }
    faceapi.draw.drawDetections(canvas, resizedDetections);
    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
  };

  const handleReferenceImageUpload = async (e) => {
    const file = e.target.files[0];
    if (file) {
      const reader = new FileReader();
      reader.onloadend = async () => {
        setReferenceImage(reader.result);
  
        // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
        const img = document.createElement("img");
        img.src = reader.result;
        img.onload = async () => {
          const canvas = canvasInRef.current;

          const displaySize = { width: img.width, height: img.height };
          faceapi.matchDimensions(canvas, displaySize);
  
          // ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
          const referenceDetection = await faceapi
            .detectSingleFace(img, new faceapi.SsdMobilenetv1Options())
            .withFaceLandmarks()
            .withFaceDescriptor();
  
          if (!referenceDetection) {
            console.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö!");
            setMatchResult("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö!");
            return;
          }
  
          // ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏≠‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡∏ô canvas
          const resizedDetection = faceapi.resizeResults([referenceDetection], displaySize);
          const ctx = canvas.getContext("2d");
          if (ctx) {
            ctx.clearRect(0, 0, canvas.width, canvas.height); // ‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå canvas ‡∏Å‡πà‡∏≠‡∏ô
            ctx.lineWidth = 10;
            faceapi.draw.drawDetections(canvas, resizedDetection);
            faceapi.draw.drawFaceLandmarks(canvas, resizedDetection);
          }
        };
      };
      reader.readAsDataURL(file);
    }
  };
  
  return (
    <div className="w-full my-4">
      {/* ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö */}
      <div className="flex gap-2 justify-around">
      <div className="">
        <input
          type="file"
          accept="image/*"
          onChange={handleReferenceImageUpload}
          className="my-2 file:bg-gray-100 file:border-none file:p-2 file:rounded-md hover:cursor-pointer file:cursor-pointer"
          placeholder="‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏û"
        />
        {referenceImage && (
          <div className="my-2 relative">
            <img
              id="reference-photo"
              src={referenceImage}
              alt="Reference"
              className="max-w-[25vw] shadow-md rounded-md"
            />
            <canvas
              ref={canvasInRef}
              className="absolute top-0 left-0 max-w-[25vw] "
            />
          </div>
        )}

              {/* ‚úÖ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö */}
      {matchResult && (
        <div className="mt-4 p-4 bg-gray-100 rounded-md">
          <p className="text-lg font-semibold">‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:</p>
          <p>{matchResult}</p>
        </div>
      )}
      </div>

      <div className="w-[35vw]">
        <Camera aspectRatio={4 / 3} ref={camera} />
        <button
          className={`bg-blue-gray-800 text-white p-4 w-full my-2 rounded-md font-semibold ${
            !isModelLoaded || !referenceImage ? "opacity-50 cursor-not-allowed" : ""
          }`}
          onClick={handleTakePhoto}
          disabled={!isModelLoaded || !referenceImage}
        >
          {isModelLoaded ? "üì∏ ‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û" : "‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•..."}
        </button>
        <button
          className="bg-red-800 text-white p-4 w-full my-2 rounded-md font-semibold"
          onClick={() => {
            setImage(null)
            setMatchResult(null)
          }}
        >
          ‡∏•‡∏ö‡∏†‡∏≤‡∏û
        </button>
      </div>
      </div>

      <hr />
      {image && (
        <div className="relative">
          <img
            id="taken-photo"
            className="absolute right-10 bottom-10 max-w-[20rem] shadow-md rounded-md cursor-pointer"
            src={image}
            alt="Taken photo"
            onClick={() => setIsOpen(true)}
            onLoad={handleImageLoad}
          />
          <canvas
            ref={canvasOutRef}
            className="absolute right-10 bottom-10 max-w-[20rem]"
            onClick={() => setIsOpen(true)}
          />
        </div>
      )}

      {/* ‚úÖ Modal ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≠ */}
      {isOpen && (
        <div
          className="fixed inset-0 backdrop-blur-sm bg-black/70 flex justify-center items-center z-50"
          onClick={() => setIsOpen(false)}
        >
          <img
            className="max-w-full max-h-full rounded-md shadow-lg"
            src={image || ""}
            alt="Full Size"
          />
          <canvas
            ref={canvasOutRef}
            className="absolute right-0 left-0 mx-auto z-[60] max-w-[20rem]"
            // onClick={() => setIsOpen(true)}
          />
        </div>
      )}
    </div>
  );
};

export default FaceDetection;